{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7b58b500e937f7d0f80b1920b86b86d054634e7"
   },
   "source": [
    "## Métricas e Validação do modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "Vamos explorar um conjunto de dados diferentes: uma base de vinhos; separar os dados e verificar como a mudança de parâmetros afeta o resultado. Há uma forma automatizada de fazer isso, mas isso será conteúdo do próximo encontro. Por enquanto, vamos fazer tudo manualmente para entender o processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()\n",
    "\n",
    "data = wine.data\n",
    "target = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "90beb882502d5f323484877012f765f1c0b230c5"
   },
   "outputs": [],
   "source": [
    "# separando os dados em treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=42)\n",
    "\n",
    "# separando o conjunto de treino em validação também\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "215594583e93cda8d295411b27b1a49f4331da2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia:  0.65\n"
     ]
    }
   ],
   "source": [
    "# treinando o modelo \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# avaliando o modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = knn.predict(X_val)\n",
    "print(\"Acurácia: \", accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Matriz de confusão',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('Gabarito')\n",
    "    plt.xlabel('Predição')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# calcula a matriz de confusão\n",
    "cnf_matrix = confusion_matrix(y_val, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# não normalizada\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=np.unique(wine.target),\n",
    "                      title='Matriz de confusão sem normalização')\n",
    "\n",
    "# normalizada\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=np.unique(wine.target), normalize=True,\n",
    "                      title='Matriz de confusão normalizada')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80        13\n",
      "           1       0.64      0.60      0.62        15\n",
      "           2       0.50      0.58      0.54        12\n",
      "\n",
      "   micro avg       0.65      0.65      0.65        40\n",
      "   macro avg       0.66      0.65      0.65        40\n",
      "weighted avg       0.66      0.65      0.65        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "a0d03491ed77beecc60bb141d58a5b787148896d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 1 - ACC: 0.725\n",
      "K: 2 - ACC: 0.675\n",
      "K: 3 - ACC: 0.65\n",
      "K: 4 - ACC: 0.625\n",
      "K: 5 - ACC: 0.675\n",
      "\n",
      "Melhor modelo:\n",
      "K: 1 - ACC: 77.96610169491525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        20\n",
      "           1       0.79      0.79      0.79        24\n",
      "           2       0.60      0.60      0.60        15\n",
      "\n",
      "   micro avg       0.78      0.78      0.78        59\n",
      "   macro avg       0.76      0.76      0.76        59\n",
      "weighted avg       0.78      0.78      0.78        59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for k in [1,2,3,4,5]:\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors = k) # a cada passo, o parâmetro assume um valor\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = knn.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    print('K:', k, '- ACC:', acc)\n",
    "    \n",
    "    if acc > best_accuracy:\n",
    "        best_model = knn\n",
    "        best_accuracy = acc\n",
    "        \n",
    "y_pred = best_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print()\n",
    "print('Melhor modelo:')\n",
    "print('K:', best_model.get_params()['n_neighbors'], '- ACC:', acc * 100)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a2845271357a72b629d89fc13270f34267a64c5"
   },
   "outputs": [],
   "source": [
    "# embaralhando os dados várias vezes e re-executando o experimento\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) # 5 execuções diferentes com 20% dos dados para teste\n",
    "\n",
    "acc = []\n",
    "for train_index, test_index in ss.split(data):\n",
    "    knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "    knn.fit(data[train_index],target[train_index])\n",
    "    y_pred = knn.predict(data[test_index])\n",
    "    acc.append(accuracy_score(y_pred,target[test_index]))\n",
    "\n",
    "acc = np.asarray(acc) # converte pra numpy pra ficar mais simples de usar média e desvio padrão\n",
    "print('Acurácia: %.2f +- %.2f' % (acc.mean() * 100, acc.std() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "96a2df397e8bde8543f3bb4318b6811ba0a89a11"
   },
   "outputs": [],
   "source": [
    "# utilizando validação cruzada com cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "scores = cross_val_score(knn, data, target, cv=5) # 5 execuções diferentes com 20% dos dados para teste\n",
    "\n",
    "print('Acurácia: %.2f +- %.2f' % (scores.mean() * 100, scores.std() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d79b7fb7b5eef80469665473ff3fc0cf6312bfce"
   },
   "outputs": [],
   "source": [
    "# utilizando validação cruzada com KFold\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 5)\n",
    "\n",
    "acc = []\n",
    "for train_index, test_index in kf.split(data):\n",
    "    knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "    knn.fit(data[train_index],target[train_index])\n",
    "    y_pred = knn.predict(data[test_index])\n",
    "    acc.append(accuracy_score(y_pred,target[test_index]))\n",
    "\n",
    "acc = np.asarray(acc) # converte pra numpy pra ficar mais simples de usar média e desvio padrão\n",
    "print('Acurácia: %.2f +- %.2f' % (acc.mean() * 100, acc.std() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "83b26d7af62d5a09609f91d6858e7288713af332"
   },
   "outputs": [],
   "source": [
    "# utilizando validação cruzada com KFold estratificado\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "acc = []\n",
    "for train_index, test_index in kf.split(data, target): # precisa passar as classes agora para que a divisão aconteça\n",
    "    knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "    knn.fit(data[train_index],target[train_index])\n",
    "    y_pred = knn.predict(data[test_index])\n",
    "    acc.append(accuracy_score(y_pred,target[test_index]))\n",
    "\n",
    "acc = np.asarray(acc) # converte pra numpy pra ficar mais simples de usar média e desvio padrão\n",
    "print('Accuracy - %.2f +- %.2f' % (acc.mean() * 100, acc.std() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9259debcdab71e96ad682b664727fa91563b156d"
   },
   "outputs": [],
   "source": [
    "# visualizacao dos dados para entender a complexidade\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit_transform(data)\n",
    "\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], wine.target_names):\n",
    "    plt.scatter(X_r[target == i, 0], X_r[target == i, 1], color=color, alpha=.8, label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of WINE dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4930607a0f558b07c5e889effb6e8cbe9bce580b"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "data_s = scaler.transform(data)\n",
    "\n",
    "df = pd.DataFrame(data_s)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6970b71d2c70dcd33fbb9bbec5261685cd0436dc"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit_transform(data_s)\n",
    "\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], wine.target_names):\n",
    "    plt.scatter(X_r[target == i, 0], X_r[target == i, 1], color=color, alpha=.8, label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of WINE dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "91bea5e4a7edb7c32d271ab989d4f2f16f08091e"
   },
   "source": [
    "Os dados agora estão todos muito próximos de 0 com desvio padrão 1, e isso melhora a disposição gráfica como pode ser visto na imagem. Não há mais tanto aglomeração, e existe separações lineares mais visíveis. Isso são fortes indícios de que agora um método de aprendizagem terá melhor desempenho. No entanto, enquanto para visualizar o StandardScaler foi aplicado em toda a base, é importante perceber que ao testar o modelo, **o StandardScaler só pode ter dados de treino no fit**. Ou seja, ele não pode conhecer a disposição dos dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ed4e2725a261d686e726dc6aa8487fffa0e071f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "acc = []\n",
    "for train_index, test_index in kf.split(data, target): # precisa passar as classes agora para que a divisão aconteça\n",
    "    knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train = scaler.fit_transform(data[train_index]) # somente dados de treino no fit\n",
    "    test = scaler.transform(data[test_index]) # aplica-se transform no teste apenas\n",
    "    \n",
    "    knn.fit(train,target[train_index])\n",
    "    y_pred = knn.predict(test)\n",
    "    acc.append(accuracy_score(y_pred,target[test_index]))\n",
    "\n",
    "acc = np.asarray(acc) # converte pra numpy pra ficar mais simples de usar média e desvio padrão\n",
    "print('Acurácia: %.2f +- %.2f' % (acc.mean() * 100, acc.std() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizando validação cruzada com cross_val_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('clf', KNeighborsClassifier())])\n",
    "scores = cross_val_score(pipeline, data, target, cv=5) # 5 execuções diferentes com 20% dos dados para teste\n",
    "\n",
    "print('Acurácia: %.2f +- %.2f' % (scores.mean() * 100, scores.std() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ea3cea33dc00d1116d27d2c3f2deac97794704e"
   },
   "outputs": [],
   "source": [
    "# separa-se uma parcela para encontrar os melhores parâmetros (10% do original)\n",
    "data_gs, data_cv, target_gs, target_cv = train_test_split(data, target, test_size=0.9, random_state=42, stratify=target)\n",
    "\n",
    "# uma forma automática de StandardScaler + KNN\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('clf', KNeighborsClassifier())])\n",
    "\n",
    "# utiliza-se GridSearchCV para achar os melhores parâmetros\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'clf__n_neighbors': [1,2,3,4,5], 'clf__weights' : ['uniform','distance']} # quais parâmetros e quais valores serão testados\n",
    "clf = GridSearchCV(pipeline, parameters, cv=3, iid=False) # clf vai armazenar qual foi a melhor configuração\n",
    "clf.fit(data_gs, target_gs)\n",
    "\n",
    "# utilizando validação cruzada para avaliar o modelo\n",
    "scores = cross_val_score(clf, data_cv, target_cv, cv=5)\n",
    "\n",
    "print('Acurácia: %.2f +- %.2f' % (scores.mean() * 100, scores.std() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8240c0485b01f706d7d2f2476c2056614c0cf0f"
   },
   "source": [
    "# Desafio - Instruções\n",
    "\n",
    "* Com o conjunto de dados sobre *câncer de mama*, utilize a célula seguinte com os dados carregados e divididos e **obtenha o melhor desempenho em um balanceamento das métricas**. \n",
    "\n",
    "* O modelo criado por você deverá ser *testado no conjunto de teste X_test e y_test*. \n",
    "\n",
    "* *O conjunto data_train e target_train poderá ser utilizado para escolher os melhores parâmetros e validar o modelo.*\n",
    "\n",
    "* Organize e **tenha cuidado** para que seu experimento execute um *protocolo de validação que faça sentido*.\n",
    "\n",
    "Mais informações sobre esse conjunto de dados poderão ser obtidas em: \n",
    "[https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b5ab284eb392e24866598e96db40bb140310f27"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "data = breast_cancer.data\n",
    "target = breast_cancer.target\n",
    "\n",
    "X_train, y_test, X_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42, stratify=target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
